{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 132,
      "id": "Vy1WScc7_L0X",
      "metadata": {
        "id": "Vy1WScc7_L0X"
      },
      "outputs": [],
      "source": [
        "#!pip install graphein MDAnalysis torch_geometric torchmetrics wandb\n",
        "#!apt-get install dssp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "id": "e5TUgXVFkh0Z",
      "metadata": {
        "id": "e5TUgXVFkh0Z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import h5py\n",
        "import collections\n",
        "import wandb\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from enum import Enum\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Dataset, Batch, Data\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.lib.distances import calc_dihedrals\n",
        "\n",
        "from graphein.protein.config import ProteinGraphConfig, DSSPConfig\n",
        "from graphein.protein.graphs import construct_graph\n",
        "from graphein.protein.edges.distance import (\n",
        "    add_aromatic_interactions, add_disulfide_interactions,\n",
        "    add_hydrogen_bond_interactions, add_peptide_bonds,\n",
        "    add_hydrophobic_interactions, add_ionic_interactions,\n",
        "    add_k_nn_edges, add_distance_threshold\n",
        ")\n",
        "from graphein.protein.features.nodes.amino_acid import amino_acid_one_hot\n",
        "from graphein.protein.features.nodes import asa, rsa\n",
        "from graphein.protein.features.nodes.dssp import secondary_structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "id": "33YVd2x0kjLn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33YVd2x0kjLn",
        "outputId": "9463fd42-3d16-4777-d6e5-8d66e0d870e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_ROOT = \"/content/drive/MyDrive/protein_triplets_data\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "id": "ZOFELSfekjJY",
      "metadata": {
        "id": "ZOFELSfekjJY"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class ProjectConfig:\n",
        "    \"\"\"Centralized configuration for features, edges, and hyperparameters\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Node Features\n",
        "        self.use_coords = False\n",
        "        self.use_b_factor = False\n",
        "        self.use_amino_acid = True\n",
        "        self.use_asa = False\n",
        "        self.use_rsa = False\n",
        "        self.use_ss = False\n",
        "        self.use_backbone_dh = False\n",
        "        self.use_sidechain_dh = False\n",
        "        self.use_embedding = False\n",
        "        self.esm_dim = 1280\n",
        "\n",
        "        # Edge Types\n",
        "        self.edge_peptide = False\n",
        "        self.edge_aromatic = False\n",
        "        self.edge_disulfide = False\n",
        "        self.edge_hydrogen = False\n",
        "        self.edge_hydrophobic = False\n",
        "        self.edge_ionic = False\n",
        "        self.edge_knn = True\n",
        "        self.edge_distance = False\n",
        "        self.knn_k = 10\n",
        "        self.dist_threshold = 8.0\n",
        "\n",
        "        # Model Hyperparameters\n",
        "        self.hidden_dim = 64\n",
        "        self.output_dim = 256\n",
        "        self.heads = 4\n",
        "        self.dropout = 0.1\n",
        "        self.lr = 0.0005\n",
        "        self.batch_size = 8\n",
        "        self.epochs = 50\n",
        "        self.margin = 0.2\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        \"\"\"Calculate input dimension based on active features\"\"\"\n",
        "        dim = 0\n",
        "        if self.use_coords: dim += 3\n",
        "        if self.use_b_factor: dim += 1\n",
        "        if self.use_amino_acid: dim += 20\n",
        "        if self.use_asa: dim += 1\n",
        "        if self.use_rsa: dim += 1\n",
        "        if self.use_ss: dim += 8\n",
        "        if self.use_backbone_dh: dim += 3\n",
        "        if self.use_sidechain_dh: dim += 5\n",
        "        if self.use_embedding: dim += self.esm_dim\n",
        "        return dim\n",
        "\n",
        "    def get_active_edge_funcs(self):\n",
        "        \"\"\"Return list of active edge construction functions\"\"\"\n",
        "        edge_funcs = []\n",
        "        if self.edge_peptide: edge_funcs.append(add_peptide_bonds)\n",
        "        if self.edge_aromatic: edge_funcs.append(add_aromatic_interactions)\n",
        "        if self.edge_disulfide: edge_funcs.append(add_disulfide_interactions)\n",
        "        if self.edge_hydrogen: edge_funcs.append(add_hydrogen_bond_interactions)\n",
        "        if self.edge_hydrophobic: edge_funcs.append(add_hydrophobic_interactions)\n",
        "        if self.edge_ionic: edge_funcs.append(add_ionic_interactions)\n",
        "        if self.edge_knn: edge_funcs.append(partial(add_k_nn_edges, k=self.knn_k))\n",
        "        if self.edge_distance:\n",
        "            edge_funcs.append(partial(add_distance_threshold, long_interaction_threshold=self.dist_threshold))\n",
        "        return edge_funcs\n",
        "\n",
        "    def get_active_node_metadata_funcs(self):\n",
        "        \"\"\"Seçilen node özelliklerine göre Graphein fonksiyonlarını döndürür.\"\"\"\n",
        "        node_funcs = []\n",
        "\n",
        "        if self.use_amino_acid:\n",
        "            node_funcs.append(amino_acid_one_hot)\n",
        "\n",
        "        if self.use_asa:\n",
        "            node_funcs.append(asa)\n",
        "\n",
        "        if self.use_rsa:\n",
        "            node_funcs.append(rsa)\n",
        "\n",
        "        if self.use_ss:\n",
        "            node_funcs.append(secondary_structure)\n",
        "\n",
        "        return node_funcs\n",
        "\n",
        "    def get_node_attributes_list(self):\n",
        "        \"\"\"Return list of active node attributes\"\"\"\n",
        "        attrs = []\n",
        "        if self.use_coords: attrs.append(\"coords\")\n",
        "        if self.use_b_factor: attrs.append(\"b_factor\")\n",
        "        if self.use_amino_acid: attrs.append(\"amino_acid_one_hot\")\n",
        "        if self.use_asa: attrs.append(\"asa\")\n",
        "        if self.use_rsa: attrs.append(\"rsa\")\n",
        "        if self.use_ss: attrs.append(\"ss\")\n",
        "        if self.use_backbone_dh: attrs.append(\"backbone_dihedral_radians\")\n",
        "        if self.use_sidechain_dh: attrs.append(\"sidechain_dihedral_radians\")\n",
        "        if self.use_embedding: attrs.append(\"embedding\")\n",
        "        return attrs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3g7MXnYEkjHi",
      "metadata": {
        "id": "3g7MXnYEkjHi"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATA MAPPER\n",
        "# ============================================================================\n",
        "\n",
        "class TripletDataPathMapper:\n",
        "    \"\"\"Maps protein file structure to anchor-positive-negative triplets with train/val/test splits\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, train_ratio=0.9, val_ratio=0.05, test_ratio=0.05, seed=42):\n",
        "        self.root_dir = root_dir\n",
        "        self.triplets = []\n",
        "        self.train_ratio = train_ratio\n",
        "        self.val_ratio = val_ratio\n",
        "        self.test_ratio = test_ratio\n",
        "        self.seed = seed\n",
        "        \n",
        "        # Split indices\n",
        "        self.train_triplets = []\n",
        "        self.val_triplets = []\n",
        "        self.test_triplets = []\n",
        "        \n",
        "        self._map_data()\n",
        "        self._split_data()\n",
        "\n",
        "    def _map_data(self):\n",
        "        originals = glob.glob(os.path.join(self.root_dir, 'originals', \"*.pdb\"))\n",
        "\n",
        "        for anchor in originals:\n",
        "            prot_id = os.path.splitext(os.path.basename(anchor))[0]\n",
        "            pos_dir = os.path.join(self.root_dir, 'positives', prot_id)\n",
        "            neg_dir = os.path.join(self.root_dir, 'negatives', prot_id)\n",
        "\n",
        "            p_files = glob.glob(os.path.join(pos_dir, \"*.pdb\"))\n",
        "            n_files = glob.glob(os.path.join(neg_dir, \"*.pdb\"))\n",
        "\n",
        "            if p_files and n_files:\n",
        "                self.triplets.append({\n",
        "                    'anchor': anchor,\n",
        "                    'positives': p_files,\n",
        "                    'negatives': n_files,\n",
        "                    'protein_id': prot_id\n",
        "                })\n",
        "\n",
        "        print(f\"Found {len(self.triplets)} protein families\")\n",
        "\n",
        "    def _split_data(self):\n",
        "        \"\"\"Split protein families into train/val/test sets\"\"\"\n",
        "        random.seed(self.seed)\n",
        "        \n",
        "        # Shuffle protein families\n",
        "        indices = list(range(len(self.triplets)))\n",
        "        random.shuffle(indices)\n",
        "        \n",
        "        n_total = len(indices)\n",
        "        n_train = int(n_total * self.train_ratio)\n",
        "        n_val = int(n_total * self.val_ratio)\n",
        "        \n",
        "        train_indices = indices[:n_train]\n",
        "        val_indices = indices[n_train:n_train + n_val]\n",
        "        test_indices = indices[n_train + n_val:]\n",
        "        \n",
        "        self.train_triplets = [self.triplets[i] for i in train_indices]\n",
        "        self.val_triplets = [self.triplets[i] for i in val_indices]\n",
        "        self.test_triplets = [self.triplets[i] for i in test_indices]\n",
        "        \n",
        "        print(f\"Split: Train={len(self.train_triplets)}, Val={len(self.val_triplets)}, Test={len(self.test_triplets)}\")\n",
        "        print(f\"Train proteins: {[t['protein_id'] for t in self.train_triplets]}\")\n",
        "        print(f\"Val proteins: {[t['protein_id'] for t in self.val_triplets]}\")\n",
        "        print(f\"Test proteins: {[t['protein_id'] for t in self.test_triplets]}\")\n",
        "    \n",
        "    def get_split(self, split='train'):\n",
        "        \"\"\"Get triplets for a specific split\"\"\"\n",
        "        if split == 'train':\n",
        "            return self.train_triplets\n",
        "        elif split == 'val':\n",
        "            return self.val_triplets\n",
        "        elif split == 'test':\n",
        "            return self.test_triplets\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown split: {split}. Use 'train', 'val', or 'test'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LAMB8c9ykjFK",
      "metadata": {
        "id": "LAMB8c9ykjFK"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class TripletProteinGraphDataset(Dataset):\n",
        "    \"\"\"PyTorch Geometric Dataset for protein triplets\"\"\"\n",
        "\n",
        "    def __init__(self, mapper, root, config: ProjectConfig, split='train', esm2_embedding_path: str = None, force: bool = False):\n",
        "        self.mapper = mapper\n",
        "        self.config = config\n",
        "        self.split = split\n",
        "        self.triplets = mapper.get_split(split) # triplets for the specified split\n",
        "        self.esm2_embedding_path = esm2_embedding_path\n",
        "\n",
        "        processed_dir = os.path.join(root, \"processed\")\n",
        "        if not os.path.exists(processed_dir):\n",
        "            os.makedirs(processed_dir)\n",
        "        if force:\n",
        "            self._clear_processed_files()\n",
        "\n",
        "        # Load embeddings if needed\n",
        "        self.esm2_embeddings = {}\n",
        "        if self.config.use_embedding and self.esm2_embedding_path and os.path.exists(self.esm2_embedding_path):\n",
        "            self._load_embeddings(self.esm2_embedding_path)\n",
        "\n",
        "        # Get active features\n",
        "        self.edge_types = self.config.get_active_edge_funcs()\n",
        "        self.node_metadata_funcs = self.config.get_active_node_metadata_funcs()\n",
        "        self.node_attributes = self.config.get_node_attributes_list()\n",
        "        self.edge_attributes = ['kind', 'edge_attr', 'euclidean_distance']\n",
        "        \n",
        "        # Store all triplets for processing (need to process all proteins regardless of split ot create .pt files)\n",
        "        self.all_triplets = mapper.triplets # triplets for all splits together\n",
        "\n",
        "        super().__init__(root)\n",
        "\n",
        "    def _clear_processed_files(self):\n",
        "        processed_dir = os.path.join(DATA_ROOT, \"processed\")\n",
        "        for path in glob.glob(os.path.join(processed_dir, \"*.pt\")):\n",
        "            os.remove(path)\n",
        "            \n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        # Process all proteins regardless of split\n",
        "        unique_paths = set()\n",
        "        for t in self.all_triplets:\n",
        "            unique_paths.add(t['anchor'])\n",
        "            unique_paths.update(t['positives'])\n",
        "            unique_paths.update(t['negatives'])\n",
        "        return [os.path.basename(p).replace(\".pdb\", \".pt\") for p in unique_paths]\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.triplets) * 10\n",
        "\n",
        "    def _load_embeddings(self, path):\n",
        "        with h5py.File(path, \"r\") as h5_file:\n",
        "            for grp in h5_file.keys():\n",
        "                self.esm2_embeddings[grp] = {}\n",
        "                for seq in h5_file[grp].keys():\n",
        "                    self.esm2_embeddings[grp][seq] = np.array(h5_file[grp][seq])\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Process all unique proteins and save to disk\"\"\"\n",
        "        unique_paths = set()\n",
        "        for t in self.all_triplets:\n",
        "            unique_paths.add(t['anchor'])\n",
        "            unique_paths.update(t['positives'])\n",
        "            unique_paths.update(t['negatives'])\n",
        "\n",
        "        print(f\"Processing {len(unique_paths)} unique proteins...\")\n",
        "\n",
        "        for path in tqdm(list(unique_paths), desc=\"Processing\"):\n",
        "            pdb_code = os.path.splitext(os.path.basename(path))[0]\n",
        "            pt_path = os.path.join(self.processed_dir, f\"{pdb_code}.pt\")\n",
        "            pickle_path = os.path.join(self.processed_dir, f\"{pdb_code}.pickle\")\n",
        "\n",
        "            if os.path.exists(pt_path) and os.path.exists(pickle_path):\n",
        "                continue\n",
        "\n",
        "\n",
        "            g = self._build_graph(path)\n",
        "            if g is None: continue\n",
        "\n",
        "            data = self._create_pyg_data(g)\n",
        "\n",
        "            with open(pickle_path, \"wb\") as f:\n",
        "                pickle.dump(g, f)\n",
        "            torch.save(data, pt_path)\n",
        "\n",
        "    def _build_graph(self, path: str):\n",
        "        \"\"\"Build protein graph from PDB file\"\"\"\n",
        "        config = ProteinGraphConfig(\n",
        "            edge_construction_functions=self.edge_types,\n",
        "            node_metadata_functions=self.node_metadata_funcs,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        g = construct_graph(config=config, path=path, verbose=False)\n",
        "        first_node = list(g.nodes())[0]\n",
        "        chain_id = first_node.split(\":\")[0]\n",
        "        pdb_code = os.path.basename(path).replace(\".pdb\", \"\")\n",
        "        g = self._process_graph(g, chain_id, path, pdb_code)\n",
        "        return g\n",
        "\n",
        "    def _process_graph(self, g, chain_id, pdb_path, pdb_code):\n",
        "        \"\"\"Process graph features\"\"\"\n",
        "        unique_edge_types = [\"peptide_bond\",\"aromatic\",\"disulfide\",\"hydrogen_bond\",\"hydrophobic\",\"ionic\",\"k_nn\",\"distance_threshold\"]\n",
        "        sequence = g.graph.get(f\"sequence_{chain_id}\")\n",
        "\n",
        "        # Process nodes\n",
        "        for index, (n, d) in enumerate(g.nodes(data=True)):\n",
        "            aa = n.split(\":\")[1]\n",
        "            d['chain_id'] = chain_id\n",
        "            d['residue_name'] = aa\n",
        "            d['residue_number'] = int(n.split(\":\")[2])\n",
        "\n",
        "            \"\"\"\n",
        "            # Clean attributes\n",
        "            for key in [\"asa\", \"rsa\", \"ss\"]:\n",
        "                if isinstance(d.get(key), pd.core.series.Series):\n",
        "                    val = d.get(key).dropna()\n",
        "                    val = list(val[val != 0].to_dict().values()) if key != \"ss\" else list(val.unique())\n",
        "                    d[key] = val[0] if val else (0.0 if key != \"ss\" else \"-\")\n",
        "\n",
        "            if not d.get(\"asa\"): d[\"asa\"] = 0\n",
        "            if not d.get(\"rsa\"): d[\"rsa\"] = 0.0\n",
        "            if not d.get(\"ss\"): d[\"ss\"] = \"-\"\n",
        "\n",
        "            d[\"ss\"] = self._one_hot_encode([d[\"ss\"]], unique_ss)[0].tolist()\n",
        "            d[\"backbone_dihedral_radians\"] = self._calc_backbone_dihedrals(pdb_path, d)\n",
        "            d[\"sidechain_dihedral_radians\"] = self._calc_sidechain_dihedrals(pdb_path, d)\n",
        "            \"\"\"\n",
        "            if self.esm2_embeddings and sequence:\n",
        "                key = f\"{pdb_code}_{chain_id}\"\n",
        "                if key not in self.esm2_embeddings: key = pdb_code\n",
        "                if key in self.esm2_embeddings:\n",
        "                    d[\"embedding\"] = self.esm2_embeddings[key][sequence][index]\n",
        "\n",
        "        # Process edges\n",
        "        for s, t, d in g.edges(data=True):\n",
        "            edge_type = list(d[\"kind\"])\n",
        "            if \"knn\" in edge_type and len(edge_type) > 1:\n",
        "                edge_type.remove(\"knn\")\n",
        "\n",
        "            d[\"edge_attr\"] = [self._one_hot_encode([_type], unique_edge_types)[0].tolist() for _type in edge_type]\n",
        "            d[\"kind\"] = edge_type\n",
        "\n",
        "            source_coords = g.nodes[s][\"coords\"]\n",
        "            target_coords = g.nodes[t][\"coords\"]\n",
        "            d[\"euclidean_distance\"] = round(np.sqrt(np.sum(np.square(source_coords - target_coords))).item(), 5)\n",
        "\n",
        "        #g = self._scale_graph(g)\n",
        "        return g\n",
        "\n",
        "    def _create_pyg_data(self, g, to_undirected_graph=True):\n",
        "        \"\"\"Convert NetworkX graph to PyTorch Geometric Data object\"\"\"\n",
        "        node_indexes_mapping = {}\n",
        "        node_features = collections.defaultdict(list)\n",
        "\n",
        "        for index, (n, d) in enumerate(g.nodes(data=True)):\n",
        "            _list = []\n",
        "            for k in self.node_attributes:\n",
        "                v = d.get(k)\n",
        "                if v is None: continue\n",
        "                if isinstance(v, (list, np.ndarray)):\n",
        "                    _list.extend(list(v))\n",
        "                else:\n",
        "                    _list.append(v)\n",
        "\n",
        "            node_features[\"x\"].append(_list)\n",
        "            node_features[\"pos\"].append(d[\"coords\"].tolist())\n",
        "            node_indexes_mapping[n] = index\n",
        "\n",
        "        edge_features = collections.defaultdict(list)\n",
        "        for s, t, d in g.edges(data=True):\n",
        "            for index, _ in enumerate(d[\"kind\"]):\n",
        "                edge_attr = []\n",
        "                edge_features[\"edge_index\"].append([node_indexes_mapping[s], node_indexes_mapping[t]])\n",
        "                edge_attr.extend(d[\"edge_attr\"][index])\n",
        "                edge_attr.append(d[\"euclidean_distance\"])\n",
        "                edge_features[\"edge_attr\"].append(edge_attr)\n",
        "\n",
        "        data = Data()\n",
        "        data.x = torch.tensor(node_features[\"x\"], dtype=torch.float)\n",
        "        data.pos = torch.tensor(node_features[\"pos\"], dtype=torch.float)\n",
        "\n",
        "        if edge_features[\"edge_index\"]:\n",
        "            data.edge_index = torch.tensor(edge_features[\"edge_index\"], dtype=torch.long).t().contiguous()\n",
        "            data.edge_attr = torch.tensor(edge_features[\"edge_attr\"], dtype=torch.float)\n",
        "        else:\n",
        "            data.edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "            data.edge_attr = torch.empty((0, 0), dtype=torch.float)\n",
        "\n",
        "        if to_undirected_graph and data.edge_index.numel() > 0:\n",
        "            data.edge_index, data.edge_attr = to_undirected(data.edge_index, data.edge_attr)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _one_hot_encode(self, classes, class_labels):\n",
        "        encoding = np.zeros((len(classes), len(class_labels)))\n",
        "        for i, class_ in enumerate(classes):\n",
        "            if class_ in class_labels:\n",
        "                encoding[i, class_labels.index(class_)] = 1\n",
        "        return encoding\n",
        "    \"\"\"\n",
        "    def _scale_graph(self, g, scale_attributes=None):\n",
        "        if scale_attributes is None:\n",
        "            scale_attributes = [\"b_factor\", \"asa\", \"rsa\"]\n",
        "\n",
        "        index_to_aa = {index: n for index, n in enumerate(g.nodes(data=False))}\n",
        "        for attr in scale_attributes:\n",
        "            vals = [d.get(attr, 0) for _, d in g.nodes(data=True)]\n",
        "            min_val, max_val = min(vals), max(vals)\n",
        "            scaled_values = [\n",
        "                round((val - min_val) / (max_val - min_val), 5) if max_val - min_val != 0 else 0\n",
        "                for val in vals\n",
        "            ]\n",
        "            scaled_dict = {index_to_aa[index]: value for index, value in enumerate(scaled_values)}\n",
        "            for n, d in g.nodes(data=True):\n",
        "                d[attr] = scaled_dict[n]\n",
        "        return g\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    def _calc_backbone_dihedrals(self, pdb_path: str, aa_props: dict, normalize: bool = True) -> list:\n",
        "        u = mda.Universe(pdb_path)\n",
        "        for res in u.residues:\n",
        "            if (res.resid == aa_props[\"residue_number\"] and\n",
        "                res.resname == aa_props[\"residue_name\"] and\n",
        "                res.segid == aa_props[\"chain_id\"]):\n",
        "\n",
        "                backbone_dihedrals_dict = {\n",
        "                    \"phi\": res.phi_selection(),\n",
        "                    \"psi\": res.psi_selection(),\n",
        "                    \"omega\": res.omega_selection()\n",
        "                }\n",
        "\n",
        "                backbone_dihedral_radians = []\n",
        "                for dihedral_selection in backbone_dihedrals_dict.values():\n",
        "                    if dihedral_selection:\n",
        "                        coords = [a.position for a in dihedral_selection.atoms]\n",
        "                        radian = calc_dihedrals(coords[0], coords[1], coords[2], coords[3])\n",
        "                        value = round(radian.item(), 5)\n",
        "                        if normalize:\n",
        "                            value = (value - (-np.pi)) / (np.pi - (-np.pi))\n",
        "                        backbone_dihedral_radians.append(round(value, 5))\n",
        "                    else:\n",
        "                        backbone_dihedral_radians.append(0.0)\n",
        "\n",
        "                return backbone_dihedral_radians\n",
        "\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\"\"\"\n",
        "    \"\"\"\n",
        "    def _calc_sidechain_dihedrals(self, pdb_path: str, aa_props: dict, normalize: bool = True) -> list:\n",
        "        chi_atoms_dict = dict(\n",
        "            chi1=dict(ARG=['N', 'CA', 'CB', 'CG'], ASN=['N', 'CA', 'CB', 'CG'], ASP=['N', 'CA', 'CB', 'CG'],\n",
        "                     CYS=['N', 'CA', 'CB', 'SG'], GLN=['N', 'CA', 'CB', 'CG'], GLU=['N', 'CA', 'CB', 'CG'],\n",
        "                     HIS=['N', 'CA', 'CB', 'CG'], ILE=['N', 'CA', 'CB', 'CG1'], LEU=['N', 'CA', 'CB', 'CG'],\n",
        "                     LYS=['N', 'CA', 'CB', 'CG'], MET=['N', 'CA', 'CB', 'CG'], PHE=['N', 'CA', 'CB', 'CG'],\n",
        "                     PRO=['N', 'CA', 'CB', 'CG'], SER=['N', 'CA', 'CB', 'OG'], THR=['N', 'CA', 'CB', 'OG1'],\n",
        "                     TRP=['N', 'CA', 'CB', 'CG'], TYR=['N', 'CA', 'CB', 'CG'], VAL=['N', 'CA', 'CB', 'CG1']),\n",
        "            chi2=dict(ARG=['CA', 'CB', 'CG', 'CD'], ASN=['CA', 'CB', 'CG', 'OD1'], ASP=['CA', 'CB', 'CG', 'OD1'],\n",
        "                     GLN=['CA', 'CB', 'CG', 'CD'], GLU=['CA', 'CB', 'CG', 'CD'], HIS=['CA', 'CB', 'CG', 'ND1'],\n",
        "                     ILE=['CA', 'CB', 'CG1', 'CD1'], LEU=['CA', 'CB', 'CG', 'CD1'], LYS=['CA', 'CB', 'CG', 'CD'],\n",
        "                     MET=['CA', 'CB', 'CG', 'SD'], PHE=['CA', 'CB', 'CG', 'CD1'], PRO=['CA', 'CB', 'CG', 'CD'],\n",
        "                     TRP=['CA', 'CB', 'CG', 'CD1'], TYR=['CA', 'CB', 'CG', 'CD1']),\n",
        "            chi3=dict(ARG=['CB', 'CG', 'CD', 'NE'], GLN=['CB', 'CG', 'CD', 'OE1'], GLU=['CB', 'CG', 'CD', 'OE1'],\n",
        "                     LYS=['CB', 'CG', 'CD', 'CE'], MET=['CB', 'CG', 'SD', 'CE']),\n",
        "            chi4=dict(ARG=['CG', 'CD', 'NE', 'CZ'], LYS=['CG', 'CD', 'CE', 'NZ']),\n",
        "            chi5=dict(ARG=['CD', 'NE', 'CZ', 'NH1'])\n",
        "            )\n",
        "\n",
        "        u = mda.Universe(pdb_path)\n",
        "        for res in u.residues:\n",
        "            if (res.resid == aa_props[\"residue_number\"] and\n",
        "                res.resname == aa_props[\"residue_name\"] and\n",
        "                res.segid == aa_props[\"chain_id\"]):\n",
        "\n",
        "                chi_radians = []\n",
        "                for chi_res in chi_atoms_dict.values():\n",
        "                    if chi_res.get(res.resname) and set(chi_res[res.resname]).issubset(set(a.name for a in res.atoms)):\n",
        "                        chi_selected_atoms = dict.fromkeys(chi_res[res.resname], 1)\n",
        "                        for a in res.atoms:\n",
        "                            if chi_selected_atoms.get(a.name) is not None and not isinstance(chi_selected_atoms.get(a.name), np.ndarray):\n",
        "                                chi_selected_atoms[a.name] = a.position\n",
        "\n",
        "                        coords = list(chi_selected_atoms.values())\n",
        "                        radian = calc_dihedrals(coords[0], coords[1], coords[2], coords[3])\n",
        "                        value = round(radian.item(), 5)\n",
        "                        if normalize:\n",
        "                            value = (value - (-np.pi)) / (np.pi - (-np.pi))\n",
        "                        chi_radians.append(round(value, 5))\n",
        "                    else:\n",
        "                        chi_radians.append(0.0)\n",
        "\n",
        "                while len(chi_radians) < 5:\n",
        "                    chi_radians.append(0.0)\n",
        "\n",
        "                return chi_radians\n",
        "\n",
        "        return [0.0] * 5\n",
        "        \"\"\"\n",
        "    def _load_processed_graph(self, path):\n",
        "        pdb_code = os.path.splitext(os.path.basename(path))[0]\n",
        "        pt_path = os.path.join(self.processed_dir, f\"{pdb_code}.pt\")\n",
        "\n",
        "        if os.path.exists(pt_path):\n",
        "            return torch.load(pt_path, weights_only=False)\n",
        "        else:\n",
        "            print(f\"Warning: {pdb_code} not found, processing on the fly\")\n",
        "            g = self._build_graph(path)\n",
        "            return self._create_pyg_data(g) if g else None\n",
        "\n",
        "    def get(self, idx):\n",
        "        real_idx = idx % len(self.triplets)\n",
        "        t = self.triplets[real_idx]\n",
        "\n",
        "        data_a = self._load_processed_graph(t[\"anchor\"])\n",
        "        data_p = self._load_processed_graph(random.choice(t[\"positives\"]))\n",
        "        data_n = self._load_processed_graph(random.choice(t[\"negatives\"]))\n",
        "\n",
        "        return data_a, data_p, data_n\n",
        "\n",
        "    def download(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "id": "nPPLoOuwkjDL",
      "metadata": {
        "id": "nPPLoOuwkjDL"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class DeepProteinGAT(nn.Module):\n",
        "    \"\"\"3-layer GATv2 model for protein embedding\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4, edge_dim = 9):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = GATv2Conv(input_dim, hidden_dim, heads=heads, concat=True, dropout=0.0, edge_dim = edge_dim)\n",
        "        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=0.0, edge_dim = edge_dim)\n",
        "        self.conv3 = GATv2Conv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.0, edge_dim = edge_dim)\n",
        "        self.projection = nn.Linear(output_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr , data.batch\n",
        "\n",
        "        x = x.float()\n",
        "        x = F.elu(self.conv1(x, edge_index, edge_attr))\n",
        "        x = F.elu(self.conv2(x, edge_index, edge_attr))\n",
        "        x = self.conv3(x, edge_index, edge_attr)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.projection(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "id": "r0SFhnYHkjAh",
      "metadata": {
        "id": "r0SFhnYHkjAh"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def triplet_collate(data_list):\n",
        "    \"\"\"Collate function for triplet batches\"\"\"\n",
        "    data_list = [x for x in data_list if x is not None]\n",
        "    if not data_list:\n",
        "        return None\n",
        "\n",
        "    batch_a = Batch.from_data_list([x[0] for x in data_list])\n",
        "    batch_p = Batch.from_data_list([x[1] for x in data_list])\n",
        "    batch_n = Batch.from_data_list([x[2] for x in data_list])\n",
        "\n",
        "    return batch_a, batch_p, batch_n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UYvT276jki-X",
      "metadata": {
        "id": "UYvT276jki-X"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "\"\"\"\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    #Evaluate model on a given dataloader\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    valid_batches = 0\n",
        "    all_pos_dist = []\n",
        "    all_neg_dist = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            if batch is None:\n",
        "                continue\n",
        "            \n",
        "            ba, bp, bn = batch\n",
        "            ba, bp, bn = ba.to(device), bp.to(device), bn.to(device)\n",
        "            \n",
        "            ea = model(ba)\n",
        "            ep = model(bp)\n",
        "            en = model(bn)\n",
        "            \n",
        "            loss = criterion(ea, ep, en)\n",
        "            total_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "            \n",
        "            dist_pos = F.pairwise_distance(ea, ep)\n",
        "            dist_neg = F.pairwise_distance(ea, en)\n",
        "            all_pos_dist.extend(dist_pos.cpu().tolist())\n",
        "            all_neg_dist.extend(dist_neg.cpu().tolist())\n",
        "    \n",
        "    avg_loss = total_loss / valid_batches if valid_batches > 0 else 0\n",
        "    avg_pos_dist = np.mean(all_pos_dist) if all_pos_dist else 0\n",
        "    avg_neg_dist = np.mean(all_neg_dist) if all_neg_dist else 0\n",
        "    \n",
        "    return avg_loss, avg_pos_dist, avg_neg_dist\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def train_pipeline(config=None, force=False):\n",
        "    \"\"\"Main training pipeline with train/val/test splits\"\"\"\n",
        "\n",
        "    # Initialize config\n",
        "    cfg = ProjectConfig()\n",
        "\n",
        "    # Initialize WandB\n",
        "    run = wandb.init(\n",
        "        project=\"ContVAR-Project\",\n",
        "        config=vars(cfg),\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # Update config from wandb if sweep is running\n",
        "    if config:\n",
        "        for key, value in config.items():\n",
        "            if hasattr(cfg, key):\n",
        "                setattr(cfg, key, value)\n",
        "\n",
        "    print(f\"Training with LR: {cfg.lr}, Hidden: {cfg.hidden_dim}, Heads: {cfg.heads}\")\n",
        "\n",
        "    # Load data with splits\n",
        "    mapper = TripletDataPathMapper(DATA_ROOT, train_ratio=1.0, val_ratio=0.0, test_ratio=0.0)\n",
        "    if not mapper.triplets:\n",
        "        print(\"No data found!\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    # Create datasets for each split\n",
        "    train_dataset = TripletProteinGraphDataset(mapper, root=DATA_ROOT, config=cfg, split='train', force=force)\n",
        "    # val_dataset = TripletProteinGraphDataset(mapper, root=DATA_ROOT, config=cfg, split='val', force=False)\n",
        "    # test_dataset = TripletProteinGraphDataset(mapper, root=DATA_ROOT, config=cfg, split='test', force=False)\n",
        "    \n",
        "    # Create dataloaders for each split\n",
        "    train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                              collate_fn=triplet_collate, num_workers=0)\n",
        "    # val_loader = DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "    #                         collate_fn=triplet_collate, num_workers=0)\n",
        "    # test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False,\n",
        "    #                          collate_fn=triplet_collate, num_workers=0)\n",
        "\n",
        "    print(f\"Dataset size - Train: {len(train_dataset)}\")\n",
        "\n",
        "    # Initialize model\n",
        "    model = DeepProteinGAT(\n",
        "        input_dim=cfg.input_dim,\n",
        "        hidden_dim=cfg.hidden_dim,\n",
        "        output_dim=cfg.output_dim,\n",
        "        heads=cfg.heads\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=50)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "    criterion = nn.TripletMarginLoss(margin=cfg.margin, p=2, swap=True)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    best_train_loss = float('inf')\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        model.train() # moved inside of the loop to ensure correct mode (when we have validation later with eval mode)\n",
        "        total_loss = 0\n",
        "        valid_batches = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\", leave=False)\n",
        "\n",
        "        for batch in pbar:\n",
        "            if batch is None:\n",
        "                continue\n",
        "\n",
        "            ba, bp, bn = batch\n",
        "            ba, bp, bn = ba.to(DEVICE), bp.to(DEVICE), bn.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ea = model(ba)\n",
        "            ep = model(bp)\n",
        "            en = model(bn)\n",
        "\n",
        "            loss = criterion(ea, ep, en)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            dist_pos = F.pairwise_distance(ea, ep)\n",
        "            dist_neg = F.pairwise_distance(ea, en)\n",
        "\n",
        "            wandb.log({\n",
        "                \"batch_loss\": loss.item(),\n",
        "                \"avg_pos_dist\": dist_pos.mean().item(),\n",
        "                \"avg_neg_dist\": dist_neg.mean().item()\n",
        "            })\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_train_loss = total_loss / valid_batches if valid_batches > 0 else 0\n",
        "\n",
        "        \n",
        "        # Validation phase (commented out for initial overfitting check)\n",
        "        # val_loss, val_pos_dist, val_neg_dist = evaluate(model, val_loader, criterion, DEVICE)\n",
        "\n",
        "        # Logging\n",
        "        log_dict = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_train_loss,\n",
        "            # \"val_loss\": val_loss,\n",
        "            # \"val_pos_dist\": val_pos_dist,\n",
        "            # \"val_neg_dist\": val_neg_dist,\n",
        "            \"lr\": cfg.lr\n",
        "        }\n",
        "\n",
        "        # Save best model based on training loss (use val_loss when validation is enabled)\n",
        "        if avg_train_loss < best_train_loss:\n",
        "            best_train_loss = avg_train_loss\n",
        "            model_name = f\"model_best_loss.pt\"\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"ContVAR-Best-Model-{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Best model at epoch {epoch+1} with train_loss {avg_train_loss:.4f}\"\n",
        "            )\n",
        "            artifact.add_file(model_name)\n",
        "            wandb.log_artifact(artifact)\n",
        "            log_dict[\"best_model_saved\"] = True\n",
        "\n",
        "        wandb.log(log_dict)\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {avg_train_loss:.4f} {'(Saved)' if log_dict.get('best_model_saved') else ''}\")\n",
        "\n",
        "    # Final test evaluation (commented out for initial overfitting check)\n",
        "    # print(\"\\nEvaluating on test set...\")\n",
        "    # model.load_state_dict(torch.load(\"model_best_loss.pt\", weights_only=False))\n",
        "    # test_loss, test_pos_dist, test_neg_dist = evaluate(model, test_loader, criterion, DEVICE)\n",
        "    # \n",
        "    # print(f\"Test Results - Loss: {test_loss:.4f}, Pos Dist: {test_pos_dist:.4f}, Neg Dist: {test_neg_dist:.4f}\")\n",
        "    # \n",
        "    # wandb.log({\n",
        "    #     \"test_loss\": test_loss,\n",
        "    #     \"test_pos_dist\": test_pos_dist,\n",
        "    #     \"test_neg_dist\": test_neg_dist\n",
        "    # })\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "JTy_Tj5xkizP",
      "metadata": {
        "id": "JTy_Tj5xkizP"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graph(protein_id=None):\n",
        "    \"\"\"Visualize a processed protein graph\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from graphein.protein.visualisation import plotly_protein_structure_graph\n",
        "\n",
        "    print(\"Starting visualization...\")\n",
        "\n",
        "    cfg = ProjectConfig()\n",
        "    mapper = TripletDataPathMapper(DATA_ROOT)\n",
        "\n",
        "    if not mapper.triplets:\n",
        "        print(\"No data found!\")\n",
        "        return\n",
        "\n",
        "    # Initialize WandB for visualization\n",
        "    wandb.init(\n",
        "        project=\"ContVAR-Project\",\n",
        "        name=\"Graph-Visualization\",\n",
        "        job_type=\"visualization\",\n",
        "        config=vars(cfg)\n",
        "    )\n",
        "\n",
        "    # Select protein\n",
        "    if protein_id:\n",
        "        choice = next((t for t in mapper.triplets if protein_id in t['anchor']), None)\n",
        "        if not choice:\n",
        "            print(f\"Protein {protein_id} not found!\")\n",
        "            wandb.finish()\n",
        "            return\n",
        "    else:\n",
        "        choice = random.choice(mapper.triplets)\n",
        "\n",
        "    pdb_path = choice['anchor']\n",
        "    pdb_code = os.path.splitext(os.path.basename(pdb_path))[0]\n",
        "\n",
        "    processed_dir = os.path.join(DATA_ROOT, \"processed\")\n",
        "    pickle_path = os.path.join(processed_dir, f\"{pdb_code}.pickle\")\n",
        "\n",
        "    print(f\"Visualizing: {pdb_code}\")\n",
        "\n",
        "    if not os.path.exists(pickle_path):\n",
        "        print(f\"Graph not processed yet. Run training first!\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Load graph\n",
        "        with open(pickle_path, \"rb\") as f:\n",
        "            g = pickle.load(f)\n",
        "\n",
        "        # Graph statistics\n",
        "        num_nodes = g.number_of_nodes()\n",
        "        num_edges = g.number_of_edges()\n",
        "        density = nx.density(g)\n",
        "\n",
        "        print(f\"Nodes: {num_nodes}, Edges: {num_edges}, Density: {density:.4f}\")\n",
        "\n",
        "        # Create interactive plot\n",
        "        fig = plotly_protein_structure_graph(\n",
        "            g,\n",
        "            colour_edges_by=\"kind\",\n",
        "            label_node_ids=False,\n",
        "            node_size_multiplier=1\n",
        "        )\n",
        "\n",
        "        fig.update_layout(title=f\"Graph Topology: {pdb_code}\")\n",
        "\n",
        "        # Log to WandB\n",
        "        wandb.log({\n",
        "            \"Interactive_Graph\": fig,\n",
        "            \"num_nodes\": num_nodes,\n",
        "            \"num_edges\": num_edges,\n",
        "            \"graph_density\": density\n",
        "        })\n",
        "\n",
        "        print(f\"Visualization uploaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "7SB8sMg2kkA9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SB8sMg2kkA9",
        "outputId": "f28f4f1e-d962-4b89-916e-71f6bd22dbea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 142,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# Login to WandB\n",
        "wandb.login()\n",
        "#API KEY: 2becafa4dcb70173759a7b50ee5de92401c637c4\n",
        "\n",
        "#MODE = \"train\"  # Options: \"train\", \"visualize\"\n",
        "\n",
        "#if MODE == \"train\":\n",
        "    #train_pipeline()\n",
        "#elif MODE == \"visualize\":\n",
        "    #visualize_graph()  # or visualize_graph(\"specific_protein_id\")\n",
        "#else:\n",
        "    #print(\"Invalid mode! Choose 'train' or 'visualize'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "FtasJBkJJqcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FtasJBkJJqcb",
        "outputId": "cc61c6e5-f62a-4ae9-9095-d92b684be72d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251216_212406-rktd2088</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/rktd2088' target=\"_blank\">polished-hill-19</a></strong> to <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/rktd2088' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/rktd2088</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training with LR: 0.0005, Hidden: 64, Heads: 4\n",
            "Found 1 protein families\n",
            "Starting training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 | Loss: 0.1935 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 | Loss: 0.1774 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 | Loss: 0.1362 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 | Loss: 0.0318 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 | Loss: 0.0552 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 | Loss: 0.0218 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 | Loss: 0.0096 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 | Loss: 0.0000 (Saved)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9 | Loss: 0.0016 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10 | Loss: 0.0112 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 11 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 12 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 13 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 14 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 15 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 16 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 17 | Loss: 0.0033 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 18 | Loss: 0.0389 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 19 | Loss: 0.0388 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 20 | Loss: 0.0113 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 21 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 22 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 23 | Loss: 0.0183 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 24 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 25 | Loss: 0.0204 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 26 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 27 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 28 | Loss: 0.0244 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 29 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 30 | Loss: 0.0081 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 31 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 32 | Loss: 0.0349 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 33 | Loss: 0.0183 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 34 | Loss: 0.0514 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 35 | Loss: 0.1170 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 36 | Loss: 0.0441 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 37 | Loss: 0.0390 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 38 | Loss: 0.1179 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 39 | Loss: 0.1139 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 40 | Loss: 0.0390 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 41 | Loss: 0.0348 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 42 | Loss: 0.1083 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 43 | Loss: 0.0685 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 44 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 45 | Loss: 0.0890 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 46 | Loss: 0.0860 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 47 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 48 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 49 | Loss: 0.0362 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 50 | Loss: 0.0000 \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_neg_dist</td><td>▁▁▁▂▄▇▇██▇▆▅▅▇▄▆▆▇▅▄▄▇▇▄▇▃▄▂▂▃▃▂▃▃▅▅▃▃▆▄</td></tr><tr><td>avg_pos_dist</td><td>▂▅█▅▅▆▅▄▅▅▅▅▄▄▄▄▄▄▃▃▃▄▃▂▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_loss</td><td>█▇▃▄▃▁▂▁▁▁▁▁▁▄▁▁▁▂▁▁▃▁▁▁▁▃▂▁▅▄▅▁▅▄▅▁▁▂▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▂▃▁▁▁▁▁▁▁▁▁▂▁▁▁▂▁▁▁▂▁▁▂▃▅▃▂▅▂▂▅▃▄▄▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_neg_dist</td><td>0.25442</td></tr><tr><td>avg_pos_dist</td><td>0.00103</td></tr><tr><td>batch_loss</td><td>0</td></tr><tr><td>best_model_saved</td><td>True</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>lr</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-hill-19</strong> at: <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/rktd2088' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/rktd2088</a><br> View project at: <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project</a><br>Synced 5 W&B file(s), 0 media file(s), 14 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251216_212406-rktd2088/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "#if you want to regenarate the .pt files please set force to true\n",
        "train_pipeline(force = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "id": "SDEl6ItnKKop",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "SDEl6ItnKKop",
        "outputId": "9cef8295-6936-40a6-ac98-75c819d5d979"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting visualization...\n",
            "Found 1 protein families\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251216_212428-l1pzi57n</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/l1pzi57n' target=\"_blank\">Graph-Visualization</a></strong> to <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/l1pzi57n' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/l1pzi57n</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Visualizing: p53\n",
            "Nodes: 776, Edges: 4510, Density: 0.0150\n",
            "Visualization uploaded successfully!\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>graph_density</td><td>▁</td></tr><tr><td>num_edges</td><td>▁</td></tr><tr><td>num_nodes</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>graph_density</td><td>0.015</td></tr><tr><td>num_edges</td><td>4510</td></tr><tr><td>num_nodes</td><td>776</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Graph-Visualization</strong> at: <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/l1pzi57n' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project/runs/l1pzi57n</a><br> View project at: <a href='https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/eneskaraarslan-hacettepe-university/ContVAR-Project</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251216_212428-l1pzi57n/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "visualize_graph()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
