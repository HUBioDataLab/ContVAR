{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "Vy1WScc7_L0X",
      "metadata": {
        "id": "Vy1WScc7_L0X"
      },
      "outputs": [],
      "source": [
        "#!pip install graphein MDAnalysis torch_geometric torchmetrics wandb\n",
        "#!apt-get install dssp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import glob\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import h5py\n",
        "import collections\n",
        "import wandb\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tqdm import tqdm\n",
        "from functools import partial\n",
        "from enum import Enum\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.data import Dataset, Batch, Data\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool\n",
        "from torch_geometric.utils import to_undirected\n",
        "\n",
        "import MDAnalysis as mda\n",
        "from MDAnalysis.lib.distances import calc_dihedrals\n",
        "\n",
        "from graphein.protein.config import ProteinGraphConfig, DSSPConfig\n",
        "from graphein.protein.graphs import construct_graph\n",
        "from graphein.protein.edges.distance import (\n",
        "    add_aromatic_interactions, add_disulfide_interactions,\n",
        "    add_hydrogen_bond_interactions, add_peptide_bonds,\n",
        "    add_hydrophobic_interactions, add_ionic_interactions,\n",
        "    add_k_nn_edges, add_distance_threshold\n",
        ")\n",
        "from graphein.protein.features.nodes.amino_acid import amino_acid_one_hot\n",
        "from graphein.protein.features.nodes import asa, rsa\n",
        "from graphein.protein.features.nodes.dssp import secondary_structure"
      ],
      "metadata": {
        "id": "e5TUgXVFkh0Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3357bd2b-93dc-49c7-c32f-45c7d9a81fb4"
      },
      "id": "e5TUgXVFkh0Z",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:MDAnalysis.coordinates.AMBER:netCDF4 is not available. Writing AMBER ncdf files will be slow.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# GLOBAL CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "DATA_ROOT = \"/content/drive/MyDrive/bitirme/model/protein_triplets_data\"\n",
        "\n",
        "print(f\"Device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33YVd2x0kjLn",
        "outputId": "f90eef1b-9fcc-4e24-a24b-0ac7d07abb99"
      },
      "id": "33YVd2x0kjLn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class ProjectConfig:\n",
        "    \"\"\"Centralized configuration for features, edges, and hyperparameters\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        # Node Features\n",
        "        self.use_coords = True\n",
        "        self.use_b_factor = True\n",
        "        self.use_amino_acid = True\n",
        "        self.use_asa = True\n",
        "        self.use_rsa = True\n",
        "        self.use_ss = True\n",
        "        self.use_backbone_dh = True\n",
        "        self.use_sidechain_dh = True\n",
        "        self.use_embedding = False\n",
        "        self.esm_dim = 1280\n",
        "\n",
        "        # Edge Types\n",
        "        self.edge_peptide = True\n",
        "        self.edge_aromatic = True\n",
        "        self.edge_disulfide = True\n",
        "        self.edge_hydrogen = True\n",
        "        self.edge_hydrophobic = True\n",
        "        self.edge_ionic = True\n",
        "        self.edge_knn = True\n",
        "        self.edge_distance = True\n",
        "        self.knn_k = 10\n",
        "        self.dist_threshold = 8.0\n",
        "\n",
        "        # Model Hyperparameters\n",
        "        self.hidden_dim = 64\n",
        "        self.output_dim = 256\n",
        "        self.heads = 4\n",
        "        self.dropout = 0.1\n",
        "        self.lr = 0.0005\n",
        "        self.batch_size = 8\n",
        "        self.epochs = 50\n",
        "        self.margin = 0.2\n",
        "\n",
        "    @property\n",
        "    def input_dim(self):\n",
        "        \"\"\"Calculate input dimension based on active features\"\"\"\n",
        "        dim = 0\n",
        "        if self.use_coords: dim += 3\n",
        "        if self.use_b_factor: dim += 1\n",
        "        if self.use_amino_acid: dim += 20\n",
        "        if self.use_asa: dim += 1\n",
        "        if self.use_rsa: dim += 1\n",
        "        if self.use_ss: dim += 8\n",
        "        if self.use_backbone_dh: dim += 3\n",
        "        if self.use_sidechain_dh: dim += 5\n",
        "        if self.use_embedding: dim += self.esm_dim\n",
        "        return dim\n",
        "\n",
        "    def get_active_edge_funcs(self):\n",
        "        \"\"\"Return list of active edge construction functions\"\"\"\n",
        "        edge_funcs = []\n",
        "        if self.edge_peptide: edge_funcs.append(add_peptide_bonds)\n",
        "        if self.edge_aromatic: edge_funcs.append(add_aromatic_interactions)\n",
        "        if self.edge_disulfide: edge_funcs.append(add_disulfide_interactions)\n",
        "        if self.edge_hydrogen: edge_funcs.append(add_hydrogen_bond_interactions)\n",
        "        if self.edge_hydrophobic: edge_funcs.append(add_hydrophobic_interactions)\n",
        "        if self.edge_ionic: edge_funcs.append(add_ionic_interactions)\n",
        "        if self.edge_knn: edge_funcs.append(partial(add_k_nn_edges, k=self.knn_k))\n",
        "        if self.edge_distance:\n",
        "            edge_funcs.append(partial(add_distance_threshold, long_interaction_threshold=self.dist_threshold))\n",
        "        return edge_funcs\n",
        "\n",
        "    def get_node_attributes_list(self):\n",
        "        \"\"\"Return list of active node attributes\"\"\"\n",
        "        attrs = []\n",
        "        if self.use_coords: attrs.append(\"coords\")\n",
        "        if self.use_b_factor: attrs.append(\"b_factor\")\n",
        "        if self.use_amino_acid: attrs.append(\"amino_acid_one_hot\")\n",
        "        if self.use_asa: attrs.append(\"asa\")\n",
        "        if self.use_rsa: attrs.append(\"rsa\")\n",
        "        if self.use_ss: attrs.append(\"ss\")\n",
        "        if self.use_backbone_dh: attrs.append(\"backbone_dihedral_radians\")\n",
        "        if self.use_sidechain_dh: attrs.append(\"sidechain_dihedral_radians\")\n",
        "        if self.use_embedding: attrs.append(\"embedding\")\n",
        "        return attrs"
      ],
      "metadata": {
        "id": "ZOFELSfekjJY"
      },
      "id": "ZOFELSfekjJY",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA MAPPER\n",
        "# ============================================================================\n",
        "\n",
        "class TripletDataPathMapper:\n",
        "    \"\"\"Maps protein file structure to anchor-positive-negative triplets\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.triplets = []\n",
        "        self._map_data()\n",
        "\n",
        "    def _map_data(self):\n",
        "        originals = glob.glob(os.path.join(self.root_dir, 'originals', \"*.pdb\"))\n",
        "\n",
        "        for anchor in originals:\n",
        "            prot_id = os.path.splitext(os.path.basename(anchor))[0]\n",
        "            pos_dir = os.path.join(self.root_dir, 'positives', prot_id)\n",
        "            neg_dir = os.path.join(self.root_dir, 'negatives', prot_id)\n",
        "\n",
        "            p_files = glob.glob(os.path.join(pos_dir, \"*.pdb\"))\n",
        "            n_files = glob.glob(os.path.join(neg_dir, \"*.pdb\"))\n",
        "\n",
        "            if p_files and n_files:\n",
        "                self.triplets.append({\n",
        "                    'anchor': anchor,\n",
        "                    'positives': p_files,\n",
        "                    'negatives': n_files\n",
        "                })\n",
        "\n",
        "        print(f\"Found {len(self.triplets)} protein families\")"
      ],
      "metadata": {
        "id": "3g7MXnYEkjHi"
      },
      "id": "3g7MXnYEkjHi",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATASET\n",
        "# ============================================================================\n",
        "\n",
        "class TripletProteinGraphDataset(Dataset):\n",
        "    \"\"\"PyTorch Geometric Dataset for protein triplets\"\"\"\n",
        "\n",
        "    def __init__(self, mapper, root, config: ProjectConfig, esm2_embedding_path: str = None):\n",
        "        self.triplets = mapper.triplets\n",
        "        self.mapper = mapper\n",
        "        self.config = config\n",
        "        self.esm2_embedding_path = esm2_embedding_path\n",
        "\n",
        "        processed_dir = os.path.join(root, \"processed\")\n",
        "        if not os.path.exists(processed_dir):\n",
        "            os.makedirs(processed_dir)\n",
        "\n",
        "        # Load embeddings if needed\n",
        "        self.esm2_embeddings = {}\n",
        "        if self.config.use_embedding and self.esm2_embedding_path and os.path.exists(self.esm2_embedding_path):\n",
        "            self._load_embeddings(self.esm2_embedding_path)\n",
        "\n",
        "        # Get active features\n",
        "        self.edge_types = self.config.get_active_edge_funcs()\n",
        "        self.node_attributes = self.config.get_node_attributes_list()\n",
        "        self.edge_attributes = ['kind', 'edge_attr', 'euclidean_distance']\n",
        "\n",
        "        super().__init__(root)\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        unique_paths = set()\n",
        "        for t in self.triplets:\n",
        "            unique_paths.add(t['anchor'])\n",
        "            unique_paths.update(t['positives'])\n",
        "            unique_paths.update(t['negatives'])\n",
        "        return [os.path.basename(p).replace(\".pdb\", \".pt\") for p in unique_paths]\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self):\n",
        "        return []\n",
        "\n",
        "    def len(self) -> int:\n",
        "        return len(self.triplets) * 10\n",
        "\n",
        "    def _load_embeddings(self, path):\n",
        "        with h5py.File(path, \"r\") as h5_file:\n",
        "            for grp in h5_file.keys():\n",
        "                self.esm2_embeddings[grp] = {}\n",
        "                for seq in h5_file[grp].keys():\n",
        "                    self.esm2_embeddings[grp][seq] = np.array(h5_file[grp][seq])\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"Process all unique proteins and save to disk\"\"\"\n",
        "        unique_paths = set()\n",
        "        for t in self.triplets:\n",
        "            unique_paths.add(t['anchor'])\n",
        "            unique_paths.update(t['positives'])\n",
        "            unique_paths.update(t['negatives'])\n",
        "\n",
        "        print(f\"Processing {len(unique_paths)} unique proteins...\")\n",
        "\n",
        "        for path in tqdm(list(unique_paths), desc=\"Processing\"):\n",
        "            pdb_code = os.path.splitext(os.path.basename(path))[0]\n",
        "            pt_path = os.path.join(self.processed_dir, f\"{pdb_code}.pt\")\n",
        "            pickle_path = os.path.join(self.processed_dir, f\"{pdb_code}.pickle\")\n",
        "\n",
        "            if os.path.exists(pt_path) and os.path.exists(pickle_path):\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                g = self._build_graph(path)\n",
        "                if g is None: continue\n",
        "\n",
        "                data = self._create_pyg_data(g)\n",
        "\n",
        "                with open(pickle_path, \"wb\") as f:\n",
        "                    pickle.dump(g, f)\n",
        "                torch.save(data, pt_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {pdb_code}: {e}\")\n",
        "\n",
        "    def _build_graph(self, path: str):\n",
        "        \"\"\"Build protein graph from PDB file\"\"\"\n",
        "        config = ProteinGraphConfig(\n",
        "            edge_construction_functions=self.edge_types,\n",
        "            node_metadata_functions=[amino_acid_one_hot],\n",
        "            graph_metadata_functions=[asa, rsa, secondary_structure],\n",
        "            dssp_config=DSSPConfig(executable=\"dssp\"),\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            g = construct_graph(config=config, path=path, verbose=False)\n",
        "            first_node = list(g.nodes())[0]\n",
        "            chain_id = first_node.split(\":\")[0]\n",
        "            pdb_code = os.path.basename(path).replace(\".pdb\", \"\")\n",
        "            g = self._process_graph(g, chain_id, path, pdb_code)\n",
        "            return g\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to construct graph for {path}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _process_graph(self, g, chain_id, pdb_path, pdb_code):\n",
        "        \"\"\"Process graph features\"\"\"\n",
        "        unique_ss = ['-', 'B', 'G', 'E', 'T', 'S', 'I', 'H']\n",
        "        unique_edge_types = ['peptide_bond', 'knn', 'hydrophobic', 'aromatic', 'hbond', 'ionic', 'disulfide']\n",
        "        sequence = g.graph.get(f\"sequence_{chain_id}\")\n",
        "\n",
        "        # Process nodes\n",
        "        for index, (n, d) in enumerate(g.nodes(data=True)):\n",
        "            aa = n.split(\":\")[1]\n",
        "            d['chain_id'] = chain_id\n",
        "            d['residue_name'] = aa\n",
        "            d['residue_number'] = int(n.split(\":\")[2])\n",
        "\n",
        "            # Clean attributes\n",
        "            for key in [\"asa\", \"rsa\", \"ss\"]:\n",
        "                if isinstance(d.get(key), pd.core.series.Series):\n",
        "                    val = d.get(key).dropna()\n",
        "                    val = list(val[val != 0].to_dict().values()) if key != \"ss\" else list(val.unique())\n",
        "                    d[key] = val[0] if val else (0.0 if key != \"ss\" else \"-\")\n",
        "\n",
        "            if not d.get(\"asa\"): d[\"asa\"] = 0\n",
        "            if not d.get(\"rsa\"): d[\"rsa\"] = 0.0\n",
        "            if not d.get(\"ss\"): d[\"ss\"] = \"-\"\n",
        "\n",
        "            d[\"ss\"] = self._one_hot_encode([d[\"ss\"]], unique_ss)[0].tolist()\n",
        "            d[\"backbone_dihedral_radians\"] = self._calc_backbone_dihedrals(pdb_path, d)\n",
        "            d[\"sidechain_dihedral_radians\"] = self._calc_sidechain_dihedrals(pdb_path, d)\n",
        "\n",
        "            if self.esm2_embeddings and sequence:\n",
        "                key = f\"{pdb_code}_{chain_id}\"\n",
        "                if key not in self.esm2_embeddings: key = pdb_code\n",
        "                if key in self.esm2_embeddings:\n",
        "                    d[\"embedding\"] = self.esm2_embeddings[key][sequence][index]\n",
        "\n",
        "        # Process edges\n",
        "        for s, t, d in g.edges(data=True):\n",
        "            edge_type = list(d[\"kind\"])\n",
        "            if \"knn\" in edge_type and len(edge_type) > 1:\n",
        "                edge_type.remove(\"knn\")\n",
        "\n",
        "            d[\"edge_attr\"] = [self._one_hot_encode([_type], unique_edge_types)[0].tolist() for _type in edge_type]\n",
        "            d[\"kind\"] = edge_type\n",
        "\n",
        "            source_coords = g.nodes[s][\"coords\"]\n",
        "            target_coords = g.nodes[t][\"coords\"]\n",
        "            d[\"euclidean_distance\"] = round(np.sqrt(np.sum(np.square(source_coords - target_coords))).item(), 5)\n",
        "\n",
        "        g = self._scale_graph(g)\n",
        "        return g\n",
        "\n",
        "    def _create_pyg_data(self, g, to_undirected_graph=True):\n",
        "        \"\"\"Convert NetworkX graph to PyTorch Geometric Data object\"\"\"\n",
        "        node_indexes_mapping = {}\n",
        "        node_features = collections.defaultdict(list)\n",
        "\n",
        "        for index, (n, d) in enumerate(g.nodes(data=True)):\n",
        "            _list = []\n",
        "            for k in self.node_attributes:\n",
        "                v = d.get(k)\n",
        "                if v is None: continue\n",
        "                if isinstance(v, (list, np.ndarray)):\n",
        "                    _list.extend(list(v))\n",
        "                else:\n",
        "                    _list.append(v)\n",
        "\n",
        "            node_features[\"x\"].append(_list)\n",
        "            node_features[\"pos\"].append(d[\"coords\"].tolist())\n",
        "            node_indexes_mapping[n] = index\n",
        "\n",
        "        edge_features = collections.defaultdict(list)\n",
        "        for s, t, d in g.edges(data=True):\n",
        "            for index, _ in enumerate(d[\"kind\"]):\n",
        "                edge_attr = []\n",
        "                edge_features[\"edge_index\"].append([node_indexes_mapping[s], node_indexes_mapping[t]])\n",
        "                edge_attr.extend(d[\"edge_attr\"][index])\n",
        "                edge_attr.append(d[\"euclidean_distance\"])\n",
        "                edge_features[\"edge_attr\"].append(edge_attr)\n",
        "\n",
        "        data = Data()\n",
        "        data.x = torch.tensor(node_features[\"x\"], dtype=torch.float)\n",
        "        data.pos = torch.tensor(node_features[\"pos\"], dtype=torch.float)\n",
        "\n",
        "        if edge_features[\"edge_index\"]:\n",
        "            data.edge_index = torch.tensor(edge_features[\"edge_index\"], dtype=torch.long).t().contiguous()\n",
        "            data.edge_attr = torch.tensor(edge_features[\"edge_attr\"], dtype=torch.float)\n",
        "        else:\n",
        "            data.edge_index = torch.empty((2, 0), dtype=torch.long)\n",
        "            data.edge_attr = torch.empty((0, 0), dtype=torch.float)\n",
        "\n",
        "        if to_undirected_graph and data.edge_index.numel() > 0:\n",
        "            data.edge_index, data.edge_attr = to_undirected(data.edge_index, data.edge_attr)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def _one_hot_encode(self, classes, class_labels):\n",
        "        encoding = np.zeros((len(classes), len(class_labels)))\n",
        "        for i, class_ in enumerate(classes):\n",
        "            if class_ in class_labels:\n",
        "                encoding[i, class_labels.index(class_)] = 1\n",
        "        return encoding\n",
        "\n",
        "    def _scale_graph(self, g, scale_attributes=None):\n",
        "        if scale_attributes is None:\n",
        "            scale_attributes = [\"b_factor\", \"asa\", \"rsa\"]\n",
        "\n",
        "        index_to_aa = {index: n for index, n in enumerate(g.nodes(data=False))}\n",
        "        for attr in scale_attributes:\n",
        "            vals = [d.get(attr, 0) for _, d in g.nodes(data=True)]\n",
        "            min_val, max_val = min(vals), max(vals)\n",
        "            scaled_values = [\n",
        "                round((val - min_val) / (max_val - min_val), 5) if max_val - min_val != 0 else 0\n",
        "                for val in vals\n",
        "            ]\n",
        "            scaled_dict = {index_to_aa[index]: value for index, value in enumerate(scaled_values)}\n",
        "            for n, d in g.nodes(data=True):\n",
        "                d[attr] = scaled_dict[n]\n",
        "        return g\n",
        "\n",
        "    def _calc_backbone_dihedrals(self, pdb_path: str, aa_props: dict, normalize: bool = True) -> list:\n",
        "        u = mda.Universe(pdb_path)\n",
        "        for res in u.residues:\n",
        "            if (res.resid == aa_props[\"residue_number\"] and\n",
        "                res.resname == aa_props[\"residue_name\"] and\n",
        "                res.segid == aa_props[\"chain_id\"]):\n",
        "\n",
        "                backbone_dihedrals_dict = {\n",
        "                    \"phi\": res.phi_selection(),\n",
        "                    \"psi\": res.psi_selection(),\n",
        "                    \"omega\": res.omega_selection()\n",
        "                }\n",
        "\n",
        "                backbone_dihedral_radians = []\n",
        "                for dihedral_selection in backbone_dihedrals_dict.values():\n",
        "                    if dihedral_selection:\n",
        "                        coords = [a.position for a in dihedral_selection.atoms]\n",
        "                        radian = calc_dihedrals(coords[0], coords[1], coords[2], coords[3])\n",
        "                        value = round(radian.item(), 5)\n",
        "                        if normalize:\n",
        "                            value = (value - (-np.pi)) / (np.pi - (-np.pi))\n",
        "                        backbone_dihedral_radians.append(round(value, 5))\n",
        "                    else:\n",
        "                        backbone_dihedral_radians.append(0.0)\n",
        "\n",
        "                return backbone_dihedral_radians\n",
        "\n",
        "        return [0.0, 0.0, 0.0]\n",
        "\n",
        "    def _calc_sidechain_dihedrals(self, pdb_path: str, aa_props: dict, normalize: bool = True) -> list:\n",
        "        chi_atoms_dict = dict(\n",
        "            chi1=dict(ARG=['N', 'CA', 'CB', 'CG'], ASN=['N', 'CA', 'CB', 'CG'], ASP=['N', 'CA', 'CB', 'CG'],\n",
        "                     CYS=['N', 'CA', 'CB', 'SG'], GLN=['N', 'CA', 'CB', 'CG'], GLU=['N', 'CA', 'CB', 'CG'],\n",
        "                     HIS=['N', 'CA', 'CB', 'CG'], ILE=['N', 'CA', 'CB', 'CG1'], LEU=['N', 'CA', 'CB', 'CG'],\n",
        "                     LYS=['N', 'CA', 'CB', 'CG'], MET=['N', 'CA', 'CB', 'CG'], PHE=['N', 'CA', 'CB', 'CG'],\n",
        "                     PRO=['N', 'CA', 'CB', 'CG'], SER=['N', 'CA', 'CB', 'OG'], THR=['N', 'CA', 'CB', 'OG1'],\n",
        "                     TRP=['N', 'CA', 'CB', 'CG'], TYR=['N', 'CA', 'CB', 'CG'], VAL=['N', 'CA', 'CB', 'CG1']),\n",
        "            chi2=dict(ARG=['CA', 'CB', 'CG', 'CD'], ASN=['CA', 'CB', 'CG', 'OD1'], ASP=['CA', 'CB', 'CG', 'OD1'],\n",
        "                     GLN=['CA', 'CB', 'CG', 'CD'], GLU=['CA', 'CB', 'CG', 'CD'], HIS=['CA', 'CB', 'CG', 'ND1'],\n",
        "                     ILE=['CA', 'CB', 'CG1', 'CD1'], LEU=['CA', 'CB', 'CG', 'CD1'], LYS=['CA', 'CB', 'CG', 'CD'],\n",
        "                     MET=['CA', 'CB', 'CG', 'SD'], PHE=['CA', 'CB', 'CG', 'CD1'], PRO=['CA', 'CB', 'CG', 'CD'],\n",
        "                     TRP=['CA', 'CB', 'CG', 'CD1'], TYR=['CA', 'CB', 'CG', 'CD1']),\n",
        "            chi3=dict(ARG=['CB', 'CG', 'CD', 'NE'], GLN=['CB', 'CG', 'CD', 'OE1'], GLU=['CB', 'CG', 'CD', 'OE1'],\n",
        "                     LYS=['CB', 'CG', 'CD', 'CE'], MET=['CB', 'CG', 'SD', 'CE']),\n",
        "            chi4=dict(ARG=['CG', 'CD', 'NE', 'CZ'], LYS=['CG', 'CD', 'CE', 'NZ']),\n",
        "            chi5=dict(ARG=['CD', 'NE', 'CZ', 'NH1'])\n",
        "            )\n",
        "\n",
        "        u = mda.Universe(pdb_path)\n",
        "        for res in u.residues:\n",
        "            if (res.resid == aa_props[\"residue_number\"] and\n",
        "                res.resname == aa_props[\"residue_name\"] and\n",
        "                res.segid == aa_props[\"chain_id\"]):\n",
        "\n",
        "                chi_radians = []\n",
        "                for chi_res in chi_atoms_dict.values():\n",
        "                    if chi_res.get(res.resname) and set(chi_res[res.resname]).issubset(set(a.name for a in res.atoms)):\n",
        "                        chi_selected_atoms = dict.fromkeys(chi_res[res.resname], 1)\n",
        "                        for a in res.atoms:\n",
        "                            if chi_selected_atoms.get(a.name) is not None and not isinstance(chi_selected_atoms.get(a.name), np.ndarray):\n",
        "                                chi_selected_atoms[a.name] = a.position\n",
        "\n",
        "                        coords = list(chi_selected_atoms.values())\n",
        "                        radian = calc_dihedrals(coords[0], coords[1], coords[2], coords[3])\n",
        "                        value = round(radian.item(), 5)\n",
        "                        if normalize:\n",
        "                            value = (value - (-np.pi)) / (np.pi - (-np.pi))\n",
        "                        chi_radians.append(round(value, 5))\n",
        "                    else:\n",
        "                        chi_radians.append(0.0)\n",
        "\n",
        "                while len(chi_radians) < 5:\n",
        "                    chi_radians.append(0.0)\n",
        "\n",
        "                return chi_radians\n",
        "\n",
        "        return [0.0] * 5\n",
        "\n",
        "    def _load_processed_graph(self, path):\n",
        "        pdb_code = os.path.splitext(os.path.basename(path))[0]\n",
        "        pt_path = os.path.join(self.processed_dir, f\"{pdb_code}.pt\")\n",
        "\n",
        "        if os.path.exists(pt_path):\n",
        "            return torch.load(pt_path, weights_only=False)\n",
        "        else:\n",
        "            print(f\"Warning: {pdb_code} not found, processing on the fly\")\n",
        "            g = self._build_graph(path)\n",
        "            return self._create_pyg_data(g) if g else None\n",
        "\n",
        "    def get(self, idx):\n",
        "        real_idx = idx % len(self.triplets)\n",
        "        t = self.triplets[real_idx]\n",
        "\n",
        "        data_a = self._load_processed_graph(t[\"anchor\"])\n",
        "        data_p = self._load_processed_graph(random.choice(t[\"positives\"]))\n",
        "        data_n = self._load_processed_graph(random.choice(t[\"negatives\"]))\n",
        "\n",
        "        return data_a, data_p, data_n\n",
        "\n",
        "    def download(self):\n",
        "        pass"
      ],
      "metadata": {
        "id": "LAMB8c9ykjFK"
      },
      "id": "LAMB8c9ykjFK",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MODEL\n",
        "# ============================================================================\n",
        "\n",
        "class DeepProteinGAT(nn.Module):\n",
        "    \"\"\"3-layer GATv2 model for protein embedding\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, heads=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = GATv2Conv(input_dim, hidden_dim, heads=heads, concat=True, dropout=0.0)\n",
        "        self.conv2 = GATv2Conv(hidden_dim * heads, hidden_dim, heads=heads, concat=True, dropout=0.0)\n",
        "        self.conv3 = GATv2Conv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=0.0)\n",
        "        self.projection = nn.Linear(output_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "\n",
        "        x = x.float()\n",
        "        x = F.elu(self.conv1(x, edge_index))\n",
        "        x = F.elu(self.conv2(x, edge_index))\n",
        "        x = self.conv3(x, edge_index)\n",
        "\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.projection(x)\n",
        "        x = F.normalize(x, p=2, dim=1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "nPPLoOuwkjDL"
      },
      "id": "nPPLoOuwkjDL",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UTILITIES\n",
        "# ============================================================================\n",
        "\n",
        "def triplet_collate(data_list):\n",
        "    \"\"\"Collate function for triplet batches\"\"\"\n",
        "    data_list = [x for x in data_list if x is not None]\n",
        "    if not data_list:\n",
        "        return None\n",
        "\n",
        "    batch_a = Batch.from_data_list([x[0] for x in data_list])\n",
        "    batch_p = Batch.from_data_list([x[1] for x in data_list])\n",
        "    batch_n = Batch.from_data_list([x[2] for x in data_list])\n",
        "\n",
        "    return batch_a, batch_p, batch_n"
      ],
      "metadata": {
        "id": "r0SFhnYHkjAh"
      },
      "id": "r0SFhnYHkjAh",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TRAINING PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "def train_pipeline(config=None):\n",
        "    \"\"\"Main training pipeline\"\"\"\n",
        "\n",
        "    # Initialize config\n",
        "    cfg = ProjectConfig()\n",
        "\n",
        "    # Initialize WandB\n",
        "    run = wandb.init(\n",
        "        project=\"ContVAR-Project\",\n",
        "        config=vars(cfg),\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # Update config from wandb if sweep is running\n",
        "    if config:\n",
        "        for key, value in config.items():\n",
        "            if hasattr(cfg, key):\n",
        "                setattr(cfg, key, value)\n",
        "\n",
        "    print(f\"Training with LR: {cfg.lr}, Hidden: {cfg.hidden_dim}, Heads: {cfg.heads}\")\n",
        "\n",
        "    # Load data\n",
        "    mapper = TripletDataPathMapper(DATA_ROOT)\n",
        "    if not mapper.triplets:\n",
        "        print(\"No data found!\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    # Create dataset and dataloader\n",
        "    dataset = TripletProteinGraphDataset(mapper, root=DATA_ROOT, config=cfg)\n",
        "    loader = DataLoader(dataset, batch_size=cfg.batch_size, shuffle=True,\n",
        "                       collate_fn=triplet_collate, num_workers=0)\n",
        "\n",
        "    # Initialize model\n",
        "    model = DeepProteinGAT(\n",
        "        input_dim=cfg.input_dim,\n",
        "        hidden_dim=cfg.hidden_dim,\n",
        "        output_dim=cfg.output_dim,\n",
        "        heads=cfg.heads\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    wandb.watch(model, log=\"gradients\", log_freq=50)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
        "    criterion = nn.TripletMarginLoss(margin=cfg.margin, p=2, swap=True)\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Starting training...\")\n",
        "    model.train()\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        total_loss = 0\n",
        "        valid_batches = 0\n",
        "\n",
        "        pbar = tqdm(loader, desc=f\"Epoch {epoch+1}/{cfg.epochs}\", leave=False)\n",
        "\n",
        "        for batch in pbar:\n",
        "            if batch is None: continue\n",
        "\n",
        "            ba, bp, bn = batch\n",
        "            ba, bp, bn = ba.to(DEVICE), bp.to(DEVICE), bn.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            ea = model(ba)\n",
        "            ep = model(bp)\n",
        "            en = model(bn)\n",
        "\n",
        "            loss = criterion(ea, ep, en)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            dist_pos = F.pairwise_distance(ea, ep)\n",
        "            dist_neg = F.pairwise_distance(ea, en)\n",
        "\n",
        "            wandb.log({\n",
        "                \"batch_loss\": loss.item(),\n",
        "                \"avg_pos_dist\": dist_pos.mean().item(), # Hedef: 0'a yaklaşmalı\n",
        "                \"avg_neg_dist\": dist_neg.mean().item()  # Hedef: Margin'den (0.2) büyük olmalı\n",
        "            })\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            valid_batches += 1\n",
        "            pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "        avg_loss = total_loss / valid_batches if valid_batches > 0 else 0\n",
        "\n",
        "        # Logging\n",
        "        log_dict = {\n",
        "            \"epoch\": epoch + 1,\n",
        "            \"train_loss\": avg_loss,\n",
        "            \"lr\": cfg.lr\n",
        "        }\n",
        "\n",
        "        # Save best model\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            model_name = f\"model_best_loss.pt\"\n",
        "            torch.save(model.state_dict(), model_name)\n",
        "\n",
        "            artifact = wandb.Artifact(\n",
        "                name=f\"ContVAR-Best-Model-{wandb.run.id}\",\n",
        "                type=\"model\",\n",
        "                description=f\"Best model at epoch {epoch+1} with loss {avg_loss:.4f}\"\n",
        "            )\n",
        "            artifact.add_file(model_name)\n",
        "            wandb.log_artifact(artifact)\n",
        "            log_dict[\"best_model_saved\"] = True\n",
        "\n",
        "        wandb.log(log_dict)\n",
        "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f} {'(Saved)' if log_dict.get('best_model_saved') else ''}\")\n",
        "\n",
        "    wandb.finish()\n",
        "    print(\"Training completed!\")"
      ],
      "metadata": {
        "id": "UYvT276jki-X"
      },
      "id": "UYvT276jki-X",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "def visualize_graph(protein_id=None):\n",
        "    \"\"\"Visualize a processed protein graph\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    from graphein.protein.visualisation import plotly_protein_structure_graph\n",
        "\n",
        "    print(\"Starting visualization...\")\n",
        "\n",
        "    cfg = ProjectConfig()\n",
        "    mapper = TripletDataPathMapper(DATA_ROOT)\n",
        "\n",
        "    if not mapper.triplets:\n",
        "        print(\"No data found!\")\n",
        "        return\n",
        "\n",
        "    # Initialize WandB for visualization\n",
        "    wandb.init(\n",
        "        project=\"ContVAR-Project\",\n",
        "        name=\"Graph-Visualization\",\n",
        "        job_type=\"visualization\",\n",
        "        config=vars(cfg)\n",
        "    )\n",
        "\n",
        "    # Select protein\n",
        "    if protein_id:\n",
        "        choice = next((t for t in mapper.triplets if protein_id in t['anchor']), None)\n",
        "        if not choice:\n",
        "            print(f\"Protein {protein_id} not found!\")\n",
        "            wandb.finish()\n",
        "            return\n",
        "    else:\n",
        "        choice = random.choice(mapper.triplets)\n",
        "\n",
        "    pdb_path = choice['anchor']\n",
        "    pdb_code = os.path.splitext(os.path.basename(pdb_path))[0]\n",
        "\n",
        "    processed_dir = os.path.join(DATA_ROOT, \"processed\")\n",
        "    pickle_path = os.path.join(processed_dir, f\"{pdb_code}.pickle\")\n",
        "\n",
        "    print(f\"Visualizing: {pdb_code}\")\n",
        "\n",
        "    if not os.path.exists(pickle_path):\n",
        "        print(f\"Graph not processed yet. Run training first!\")\n",
        "        wandb.finish()\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        # Load graph\n",
        "        with open(pickle_path, \"rb\") as f:\n",
        "            g = pickle.load(f)\n",
        "\n",
        "        # Graph statistics\n",
        "        num_nodes = g.number_of_nodes()\n",
        "        num_edges = g.number_of_edges()\n",
        "        density = nx.density(g)\n",
        "\n",
        "        print(f\"Nodes: {num_nodes}, Edges: {num_edges}, Density: {density:.4f}\")\n",
        "\n",
        "        # Create interactive plot\n",
        "        fig = plotly_protein_structure_graph(\n",
        "            g,\n",
        "            colour_edges_by=\"kind\",\n",
        "            label_node_ids=False,\n",
        "            node_size_multiplier=1\n",
        "        )\n",
        "\n",
        "        fig.update_layout(title=f\"Graph Topology: {pdb_code}\")\n",
        "\n",
        "        # Log to WandB\n",
        "        wandb.log({\n",
        "            \"Interactive_Graph\": fig,\n",
        "            \"num_nodes\": num_nodes,\n",
        "            \"num_edges\": num_edges,\n",
        "            \"graph_density\": density\n",
        "        })\n",
        "\n",
        "        print(f\"Visualization uploaded successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "JTy_Tj5xkizP"
      },
      "id": "JTy_Tj5xkizP",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "# Login to WandB\n",
        "wandb.login()\n",
        "#API KEY: 2becafa4dcb70173759a7b50ee5de92401c637c4\n",
        "\n",
        "#MODE = \"train\"  # Options: \"train\", \"visualize\"\n",
        "\n",
        "#if MODE == \"train\":\n",
        "    #train_pipeline()\n",
        "#elif MODE == \"visualize\":\n",
        "    #visualize_graph()  # or visualize_graph(\"specific_protein_id\")\n",
        "#else:\n",
        "    #print(\"Invalid mode! Choose 'train' or 'visualize'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SB8sMg2kkA9",
        "outputId": "3258b70e-ea9f-4c74-ae26-48f20d54b0cb"
      },
      "id": "7SB8sMg2kkA9",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcanerayvaz\u001b[0m (\u001b[33mcanerayvaz-hacettepe-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pipeline()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FtasJBkJJqcb",
        "outputId": "6063e64a-152d-47db-b370-befdb35110b2"
      },
      "id": "FtasJBkJJqcb",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251216_174956-7g109983</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/7g109983' target=\"_blank\">iconic-forest-37</a></strong> to <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/7g109983' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/7g109983</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with LR: 0.0005, Hidden: 64, Heads: 4\n",
            "Found 1 protein families\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Loss: 0.1946 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Loss: 0.1818 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Loss: 0.1397 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Loss: 0.0481 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Loss: 0.1000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 | Loss: 0.1530 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 | Loss: 0.1576 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 | Loss: 0.1500 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 | Loss: 0.1355 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 | Loss: 0.1232 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 | Loss: 0.1086 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 | Loss: 0.0525 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 | Loss: 0.0075 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 | Loss: 0.0000 (Saved)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 | Loss: 0.0538 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 | Loss: 0.0638 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 | Loss: 0.1586 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 | Loss: 0.1656 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 | Loss: 0.1615 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Loss: 0.1484 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 21 | Loss: 0.1186 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 22 | Loss: 0.0757 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 23 | Loss: 0.0830 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 24 | Loss: 0.0705 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 25 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 26 | Loss: 0.0150 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 27 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 28 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 29 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 30 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 31 | Loss: 0.0046 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 32 | Loss: 0.0087 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 33 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 34 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 35 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 36 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 37 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 38 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 39 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 40 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 41 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 42 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 43 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 44 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 45 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 46 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 47 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 48 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 49 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 50 | Loss: 0.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_neg_dist</td><td>▁▁▂▃▄▁▁▁▂▂▃▄▄▂▁▂▂▃▃▅▆█▅▄█▆▇▇▆▆▅▅▅▅▅▅▄▄▄▄</td></tr><tr><td>avg_pos_dist</td><td>▁▁▁▄▂▂▃▄▄▄▄▂▁▂▂▃▃▃▅▅▇█▇█▇▆▆▅▅▅▄▄▄▄▄▄▄▄▄▄</td></tr><tr><td>batch_loss</td><td>█▇▇▅▂▇▇▆▆▅▅▂▁▁▁▇▇▇▇▆▃▄▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▆▃▅▇▆▆▅▅▁▁▃▃▇▇▆▅▄▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_neg_dist</td><td>0.21237</td></tr><tr><td>avg_pos_dist</td><td>0.00442</td></tr><tr><td>batch_loss</td><td>0</td></tr><tr><td>best_model_saved</td><td>True</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>lr</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">iconic-forest-37</strong> at: <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/7g109983' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/7g109983</a><br> View project at: <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project</a><br>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251216_174956-7g109983/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_graph()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "SDEl6ItnKKop",
        "outputId": "3e62eff2-aa7a-451a-b3f9-288cc55198ce"
      },
      "id": "SDEl6ItnKKop",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting visualization...\n",
            "Found 1 protein families\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251216_175018-l7f1xd2h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/l7f1xd2h' target=\"_blank\">Graph-Visualization</a></strong> to <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/l7f1xd2h' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/l7f1xd2h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizing: p53\n",
            "Nodes: 776, Edges: 4510, Density: 0.0150\n",
            "Visualization uploaded successfully!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>graph_density</td><td>▁</td></tr><tr><td>num_edges</td><td>▁</td></tr><tr><td>num_nodes</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>graph_density</td><td>0.015</td></tr><tr><td>num_edges</td><td>4510</td></tr><tr><td>num_nodes</td><td>776</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Graph-Visualization</strong> at: <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/l7f1xd2h' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project/runs/l7f1xd2h</a><br> View project at: <a href='https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project' target=\"_blank\">https://wandb.ai/canerayvaz-hacettepe-university/ContVAR-Project</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20251216_175018-l7f1xd2h/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}